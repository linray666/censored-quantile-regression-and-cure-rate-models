{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e27fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as sm\n",
    "import math\n",
    "from scipy.optimize import linprog\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import genextreme\n",
    "import progressbar\n",
    "from statsmodels.api import add_constant\n",
    "import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea62e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "widgets = [\"doing task: \", progressbar.Percentage(),\" \",progressbar.Bar(), \" \", progressbar.ETA()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61a373",
   "metadata": {},
   "source": [
    "Structure:\n",
    "1. Coefficient Estimation: Determine Tau_u, Grids\n",
    "2. Inference: Resampling for variance and mean/ Inference, second-stage inference/  \n",
    "3. Simulation\n",
    "4. Real-life analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77f822",
   "metadata": {},
   "source": [
    "Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f7cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Programming to solve coefficients\n",
    "def H(x):\n",
    "    return -np.log(1-x)\n",
    "\n",
    "def generating_grids(y):\n",
    "    n = len(y)\n",
    "    return np.arange(1/n,1-1/n,min(0.01,1/(2*n**.7)))\n",
    "\n",
    "def quantreg_fit(X,y,delta,grids):\n",
    "    p=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    if n!=len(y):\n",
    "        print(\"length not matched!\")\n",
    "        return\n",
    "    Z = np.log(y)\n",
    "    result = []\n",
    "    #Calculating coefs at different grids\n",
    "    #Initialize the recursive estimation\n",
    "    alpha = H(grids[0])*(y>= 0)\n",
    "    In = np.diag(np.repeat(1,n))\n",
    "    On_vector = np.repeat(0,n)\n",
    "    Op_vector = np.repeat(0,p) \n",
    "    var = np.array([]) \n",
    "    A = np.concatenate((X,-X,In,-In),axis = 0).transpose()\n",
    "    #Recursive estimation\n",
    "    for i in range(len(grids)-1):\n",
    "        C = np.concatenate((Op_vector,Op_vector,alpha,(delta-alpha)),axis = 0)\n",
    "        b = Z\n",
    "        l = np.repeat(0,2*n+2*p)\n",
    "        res = linprog(C, A_eq=A, b_eq=b, A_ub = -np.diag(np.repeat(1,2*n+2*p)), b_ub = l)\n",
    "        beta = res.x[:p]-res.x[p:2*p]\n",
    "        alpha = alpha + (H(grids[i+1])-H(grids[i]))*(y >= np.exp(X.transpose()@beta))\n",
    "        result.append(beta)\n",
    "    result = pd.DataFrame(result)\n",
    "    result[\"grids\"] = grids[:-1]\n",
    "    result = result.set_index(\"grids\")\n",
    "    colist = []\n",
    "    for i in range(len(result.columns.tolist())):\n",
    "        colist.append(\"beta %d\"%(result.columns.tolist()[i]))\n",
    "    result.columns = colist\n",
    "    return result\n",
    "\n",
    "def params_inference(X,y,delta,grids,B):\n",
    "    #Experiments with resamping variance and confidence interval\n",
    "    p=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    Z = np.log(y)\n",
    "    #Calculating coefs at different grids\n",
    "    #Initialize the recursive estimation\n",
    "    alpha = H(grids[0])*(y >= 0)\n",
    "    new_alpha = [alpha]*B\n",
    "    In = np.diag(np.repeat(1,n))\n",
    "    On_vector = np.repeat(0,n)\n",
    "    Op_vector = np.repeat(0,p) \n",
    "    A = np.concatenate((X,-X,In,-In),axis = 0).transpose()\n",
    "    b = Z\n",
    "    l = np.repeat(0,2*n+2*p)\n",
    "    var = [] \n",
    "    l_quantile = []\n",
    "    u_quantile = []\n",
    "    #Recursive estimation\n",
    "    random_r = np.random.exponential(size = n*B).reshape(B,n)\n",
    "    for i in range(len(grids)-1):\n",
    "        beta_stars = []#temporarily storing B numbers of beta_stars at each grid\n",
    "        for j in range(B):\n",
    "            r = random_r[j]\n",
    "            C = np.concatenate((Op_vector,Op_vector,new_alpha[j]*r,(delta-new_alpha[j])*r),axis = 0)\n",
    "            res = linprog(C, A_eq=A, b_eq=b, A_ub = -np.diag(np.repeat(1,2*n+2*p)), b_ub = l)\n",
    "            s_beta = res.x[:p]-res.x[p:2*p]\n",
    "            #Update beta and alpha\n",
    "            beta_stars.append(s_beta)\n",
    "            new_alpha[j] =  new_alpha[j] + (H(grids[i+1])-H(grids[i]))*(y >= np.exp(X.transpose()@s_beta))\n",
    "\n",
    "        beta_stars = pd.DataFrame(beta_stars)\n",
    "        var.append(beta_stars.var())\n",
    "        l_quantile.append(beta_stars.quantile(0.025))\n",
    "        u_quantile.append(beta_stars.quantile(0.975))\n",
    "    \n",
    "    collist = []\n",
    "    for i in range(p):\n",
    "        collist.append(\"beta %d\"%(i))\n",
    "    \n",
    "    var = pd.DataFrame(var)\n",
    "    l_quantile = pd.DataFrame(l_quantile).reset_index(drop = True)\n",
    "    u_quantile = pd.DataFrame(u_quantile).reset_index(drop = True)\n",
    "\n",
    "    for df in [var, l_quantile,u_quantile]:\n",
    "        df.columns = collist\n",
    "        df[\"grids\"] = grids[:-1]\n",
    "\n",
    "    params_result = pd.concat([var,l_quantile,u_quantile],keys=['var', 'lower', 'upper']).reset_index(level=1,drop = True)\n",
    "\n",
    "    return params_result\n",
    "\n",
    "def argdic_quantreg_fit(argdics):\n",
    "    return quantreg_fit(argdics[\"X\"],argdics[\"Y\"],argdics[\"delta\"],argdics[\"grids\"])\n",
    "\n",
    "def argdic_params_inference(argdics):\n",
    "    return params_inference(argdics[\"X\"],argdics[\"Y\"],argdics[\"delta\"],argdics[\"grids\"],argdics[\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f02fca1d-786e-4722-8903-96d09fcf0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T1_hypothesis_testing(X,y,delta,grids,B,l_tau,u_tau,r_v, r_0,weight_func):\n",
    "    betas_hat = quantreg_fit(X,y,delta,grids)\n",
    "    #Get T1\n",
    "    T1 = 0\n",
    "    p=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    Z = np.log(y)\n",
    "    T1_stars = [0] * B\n",
    "    #Calculating coefs at different grids\n",
    "    #Initialize the recursive estimation\n",
    "    alpha = H(grids[0])*(y >= 0)\n",
    "    new_alpha = [alpha]*B\n",
    "    In = np.diag(np.repeat(1,n))\n",
    "    On_vector = np.repeat(0,n)\n",
    "    Op_vector = np.repeat(0,p) \n",
    "    A = np.concatenate((X,-X,In,-In),axis = 0).transpose()\n",
    "    b = Z\n",
    "    l = np.repeat(0,2*n+2*p)\n",
    "\n",
    "    #Recursive estimation\n",
    "    random_r = np.random.exponential(size = n*B).reshape(B,n)\n",
    "    for i in range(len(grids)-1):\n",
    "        grid = grids[i]\n",
    "        new_r_v = r_v(grid)\n",
    "        beta_hat = betas_hat.loc[grid]\n",
    "        if (grid >= l_tau) and (grid <= u_tau):\n",
    "            T1 = T1 + (beta_hat@new_r_v - r_0(grid))*weight_func(grid)*0.01\n",
    "        for j in range(B):\n",
    "            r = random_r[j]\n",
    "            C = np.concatenate((Op_vector,Op_vector,new_alpha[j]*r,(delta-new_alpha[j])*r),axis = 0)\n",
    "            res = linprog(C, A_eq=A, b_eq=b, A_ub = -np.diag(np.repeat(1,2*n+2*p)), b_ub = l)\n",
    "            s_beta = res.x[:p]-res.x[p:2*p]\n",
    "            #Update beta and alpha\n",
    "            new_alpha[j] =  new_alpha[j] + (H(grids[i+1])-H(grids[i]))*(y >= np.exp(X.transpose()@s_beta))\n",
    "            # updating T1_stars\n",
    "            if (grid >= l_tau) and (grid <= u_tau):\n",
    "                T1_stars[j] = T1_stars[j] + (s_beta@new_r_v- beta_hat @new_r_v)*weight_func(grid)*0.01\n",
    "    T1 = T1 * np.sqrt(n)\n",
    "    T1_stars = pd.DataFrame(T1_stars)* np.sqrt(n)\n",
    "    l_quantile = T1_stars.quantile(0.025)\n",
    "    u_quantile = T1_stars.quantile(0.975)\n",
    "    return {\"T1\":T1,\"lower bound\":l_quantile,\"upper bound\": u_quantile}\n",
    "\n",
    "def argdic_T1_hypothesis_testing(argdics):\n",
    "    return T1_hypothesis_testing(argdics[\"X\"],argdics[\"Y\"],argdics[\"delta\"],argdics[\"grids\"],argdics[\"B\"],argdics[\"l_tau\"],argdics[\"u_tau\"],argdics[\"r_v\"], argdics[\"r_0\"],argdics[\"weight_func\"])\n",
    "\n",
    "def T1_hypothesis_testing_ERR(argdics):\n",
    "    res = argdic_T1_hypothesis_testing(argdics)\n",
    "    return (res[\"T1\"]>res[\"lower bound\"]) *(res[\"T1\"]<res[\"upper bound\"]) \n",
    "\n",
    "\n",
    "def average_cov_effect(betas_hat,grids,B,l_tau,u_tau):\n",
    "    return (betas_hat.loc[l_tau:u_tau].sum()*0.01)/(u_tau - l_tau)\n",
    "\n",
    "def cov_effect_var(X,y,delta,grids,B,l_tau,u_tau):\n",
    "    p=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    Z = np.log(y)\n",
    "    rhos_star = [0] * B\n",
    "    #Calculating coefs at different grids\n",
    "    #Initialize the recursive estimation\n",
    "    alpha = H(grids[0])*(y >= 0)\n",
    "    new_alpha = [alpha]*B\n",
    "    In = np.diag(np.repeat(1,n))\n",
    "    On_vector = np.repeat(0,n)\n",
    "    Op_vector = np.repeat(0,p) \n",
    "    A = np.concatenate((X,-X,In,-In),axis = 0).transpose()\n",
    "    b = Z\n",
    "    l = np.repeat(0,2*n+2*p)\n",
    "\n",
    "    #Recursive estimation\n",
    "    random_r = np.random.exponential(size = n*B).reshape(B,n)\n",
    "    for i in range(len(grids)-1):\n",
    "        grid = grids[i]\n",
    "        new_r_v = r_v(grid)\n",
    "        for j in range(B):\n",
    "            r = random_r[j]\n",
    "            C = np.concatenate((Op_vector,Op_vector,new_alpha[j]*r,(delta-new_alpha[j])*r),axis = 0)\n",
    "            res = linprog(C, A_eq=A, b_eq=b, A_ub = -np.diag(np.repeat(1,2*n+2*p)), b_ub = l)\n",
    "            s_beta = res.x[:p]-res.x[p:2*p]\n",
    "            #Update beta and alpha\n",
    "            new_alpha[j] =  new_alpha[j] + (H(grids[i+1])-H(grids[i]))*(y >= np.exp(X.transpose()@s_beta))\n",
    "            if (grid >= l_tau) and (grid <= u_tau):\n",
    "                rhos_star[j] = rhos_star[j] + s_beta * 0.01\n",
    "    rhos_star = pd.DataFrame(rhos_star)/(u_tau - l_tau)\n",
    "    return rhos_star.var()\n",
    "\n",
    "def argdic_cov_effect_var(argdics):\n",
    "    return cov_effect_var(argdics[\"X\"],argdics[\"Y\"],argdics[\"delta\"],argdics[\"grids\"],argdics[\"B\"],argdics[\"l_tau\"],argdics[\"u_tau\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f3716",
   "metadata": {},
   "source": [
    "## timing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da095150-2c50-4b39-84db-36d3fe8ead24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序运行时间:116.70840954780579秒\n"
     ]
    }
   ],
   "source": [
    "T1 = time.time()\n",
    "n=200\n",
    "b1 = .5\n",
    "b2 = -.5\n",
    "grids = np.arange(0.01, 0.72, 0.01)\n",
    "C_u = 3.8\n",
    "B = 250\n",
    "Z1 = np.random.random(n)\n",
    "Z2 = np.array([])\n",
    "C = np.array([])\n",
    "delta = np.array([])\n",
    "epsilon = -genextreme.rvs(1, size = n)\n",
    "C_u = 5\n",
    "for i in range(n):\n",
    "    z = bernoulli.rvs(.5)\n",
    "    Z2 = np.append(Z2,z)\n",
    "    if z == 0:\n",
    "        C= np.append(C, np.random.uniform(0,C_u))\n",
    "    else:\n",
    "        C= np.append(C, np.random.uniform(0.1,C_u))\n",
    "T = np.exp(b1*Z1+b2*Z2+epsilon)\n",
    "X = np.vstack((Z1,Z2,np.repeat(1,n)))\n",
    "delta = (T<=C)\n",
    "Y = np.minimum(T,C)\n",
    "params = quantreg_fit(X,Y,delta,grids)\n",
    "inference = params_inference(X,Y,delta,grids,B)\n",
    "T2 = time.time()\n",
    "print('程序运行时间:%s秒' % ((T2 - T1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed887c4",
   "metadata": {},
   "source": [
    "Simulation\n",
    "1. AFT model\n",
    "$$Log(T) = b_1Z_1+b_2Z_2+\\epsilon$$\n",
    "$$\\epsilon ～ GEV;Z_1～unif(0,1);Z_2～Bernolli(0.5) $$\n",
    "$$ if Z_2 = 0,  C～unif(0,c_u); if Z_2 = 1,  C～unif(0.1,c_u) $$\n",
    "\n",
    "1.1 \n",
    "$$c_u = 3.8, b_1 = 0.5, b_2 = -0.5;$$\n",
    "1.2\n",
    "$$C_u = 5, b_1 = 0, b_2 = -0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69c501c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Setting one\n",
    "def r_v(tau):\n",
    "    return (np.array([[1,0,0],[0,1,0]]).transpose())\n",
    "\n",
    "def r_0(tau):\n",
    "    return np.array([0,0])\n",
    "\n",
    "def weight_func(tau):\n",
    "    return 1\n",
    "\n",
    "l_tau = grids[0]\n",
    "u_tau = grids[-2]\n",
    "\n",
    "def setone_simulate(n=200,b1=.5,b2=-.5,grids=np.arange(0.01, 0.72, 0.01),C_u = 5, B = 250, l_tau=l_tau,u_tau = u_tau,r_v=r_v, r_0=r_0,weight_func =weight_func ,samplepath = 1000):\n",
    "    args = []\n",
    "    for i in range(samplepath):\n",
    "        Z1 = np.random.random(n)\n",
    "        Z2 = np.array([])\n",
    "        C = np.array([])\n",
    "        delta = np.array([])\n",
    "        epsilon = genextreme.rvs(1, size = n)\n",
    "        for j in range(n):\n",
    "            z = bernoulli.rvs(.5)\n",
    "            Z2 = np.append(Z2,z)\n",
    "            if z == 0:\n",
    "                C= np.append(C, np.random.uniform(0,C_u))\n",
    "            else:\n",
    "                C= np.append(C, np.random.uniform(0.1,C_u))\n",
    "        T = np.exp(b1*Z1+b2*Z2+epsilon)\n",
    "        X = np.vstack((Z1,Z2,np.repeat(1,n)))\n",
    "        delta = (T<=C)\n",
    "        Y = np.minimum(T,C)\n",
    "        args.append({\"X\":X,\"Y\":Y,\"grids\":grids,\"delta\":delta,\"B\":B,\"l_tau\":l_tau,\"u_tau\":u_tau,\"r_v\":r_v, \"r_0\":r_0,\"weight_func\":weight_func})\n",
    "    return args\n",
    "\n",
    "grids = np.arange(0.01, 0.72, 0.01)\n",
    "target_grids = [grids[9],grids[29],grids[49],grids[69]]\n",
    "\n",
    "def temp_quantreg(argdics):\n",
    "    return argdic_quantreg_fit(argdics).loc[target_grids]\n",
    "\n",
    "def temp_inference(argdics):\n",
    "    target_grids = [grids[9],grids[29],grids[49],grids[69]]\n",
    "    df = argdic_params_inference(argdics)\n",
    "    return df[df[\"grids\"].isin(target_grids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16d76717-e109-493f-8f36-ae2628f9aa3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序运行时间:8020699.6150016785毫秒\n"
     ]
    }
   ],
   "source": [
    "#Simulation Setting 1 Sample 1\n",
    "sample_path = 500\n",
    "sample1 = setone_simulate(samplepath = 500,B = 200, C_u = 5,b1 = 0)\n",
    "\n",
    "p = 3\n",
    "\n",
    "true_betas = []\n",
    "for i in range(4):\n",
    "    true_betas.append(np.array([0,-0.5,genextreme.ppf(target_grids[i], 1)]))\n",
    "\n",
    "#Restoring simulated samples:\n",
    "sample_betas = [np.repeat(0,p)]*4\n",
    "sample_vars = [np.repeat(0,p)]*4\n",
    "sample_CIs = [np.repeat(0,p)]*4\n",
    "dfl = []\n",
    "for i in range(4):\n",
    "    dfl.append(pd.DataFrame(columns = [\"Bias\", \"AveVar\",\"EmpVar\",\"Cov95\"]))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    T1 = time.time()\n",
    "    pool = mp.Pool(processes=40)\n",
    "    params = pool.map(temp_inference,sample1)\n",
    "    all_betas = pool.map(temp_quantreg,sample1) #a list of betas dfs\n",
    "    T2 = time.time()\n",
    "    print('程序运行时间:%s毫秒' % ((T2 - T1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80cbad63-70d6-4b87-8510-39bb19e70566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1050d24-412b-4519-9305-40e1f4f09bc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序运行时间:8174449.815273285毫秒\n"
     ]
    }
   ],
   "source": [
    "p = 3\n",
    "#Restoring simulated samples:\n",
    "sample_betas = [np.repeat(0,p)]*4\n",
    "sample_vars = [np.repeat(0,p)]*4\n",
    "sample_CIs = [np.repeat(0,p)]*4\n",
    "dfl = []\n",
    "for i in range(4):\n",
    "    dfl.append(pd.DataFrame(columns = [\"Bias\", \"AveVar\",\"EmpVar\",\"Cov95\"]))\n",
    "    \n",
    "for k in range(len(target_grids)):\n",
    "    true_beta = true_betas[k]\n",
    "    grid = target_grids[k]\n",
    "    for j in range(sample_path):\n",
    "        beta = np.array(all_betas[j].loc[grid])\n",
    "        sample_betas[k] = np.vstack((sample_betas[k], beta))\n",
    "        temp_var,temp_l,temp_u = params[j].loc[\"var\"].set_index(\"grids\"),params[j].loc[\"lower\"].set_index(\"grids\"),params[j].loc[\"upper\"].set_index(\"grids\")\n",
    "        sample_vars[k] =  np.vstack((sample_vars[k], np.array(temp_var.loc[grid]))) \n",
    "        l = true_beta - np.sqrt(np.array(temp_var.loc[grid])) * 1.96\n",
    "        u = true_beta + np.sqrt(np.array(temp_var.loc[grid])) * 1.96\n",
    "        temp = (beta>l) * (beta<u)\n",
    "        sample_CIs[k] =  np.vstack((sample_CIs[k],temp)) \n",
    "\n",
    "#Calculate empirical bias and variance\n",
    "# empirical_mean = []\n",
    "# empirical_var = []\n",
    "# empirical_coverage = []\n",
    "# average_var = []\n",
    "for k in range(4):\n",
    "    sample_betas[k] = pd.DataFrame(sample_betas[k][1:])\n",
    "    sample_vars[k] = pd.DataFrame(sample_vars[k][1:])\n",
    "    sample_CIs[k] = pd.DataFrame(sample_CIs[k][1:])\n",
    "#     empirical_mean.append(np.array(sample_betas.mean()))\n",
    "#     empirical_var.append(np.array(sample_betas.var()))\n",
    "#     empirical_coverage.append(sum(sample_CIs))\n",
    "#     average_var.append(np.array(sample_vars.mean()))\n",
    "    dfl[k][\"Bias\"] = sample_betas[k].mean() - true_betas[k] \n",
    "    dfl[k][\"AveVar\"] = sample_vars[k].mean()\n",
    "    dfl[k][\"EmpVar\"] = sample_betas[k].var()\n",
    "    dfl[k][\"Cov95\"] = sample_CIs[k].sum()/sample_path\n",
    "T2 = time.time()\n",
    "print('程序运行时间:%s毫秒' % ((T2 - T1)*1000))\n",
    "params_set1 = pd.concat(dfl,keys = [\"tau = 0.1\",\"tau = 0.3\",\"tau = 0.5\",\"tau = 0.7\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4de2b044-3675-4e80-9ac3-bc3ca4c0f278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Bias</th>\n",
       "      <th>AveVar</th>\n",
       "      <th>EmpVar</th>\n",
       "      <th>Cov95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tau = 0.1</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.015590</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.530051</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.222246</td>\n",
       "      <td>0.181487</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001507</td>\n",
       "      <td>0.275272</td>\n",
       "      <td>0.245703</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tau = 0.3</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.028725</td>\n",
       "      <td>0.163289</td>\n",
       "      <td>0.149564</td>\n",
       "      <td>0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000691</td>\n",
       "      <td>0.054713</td>\n",
       "      <td>0.047616</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025672</td>\n",
       "      <td>0.068963</td>\n",
       "      <td>0.069358</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tau = 0.5</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.009326</td>\n",
       "      <td>0.075044</td>\n",
       "      <td>0.068849</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006312</td>\n",
       "      <td>0.024788</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>0.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014723</td>\n",
       "      <td>0.031450</td>\n",
       "      <td>0.031365</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">tau = 0.7</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.004078</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>0.031756</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003606</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Bias    AveVar    EmpVar  Cov95\n",
       "tau = 0.1 0 -0.015590  0.635135  0.530051  0.950\n",
       "          1  0.013489  0.222246  0.181487  0.956\n",
       "          2 -0.001507  0.275272  0.245703  0.932\n",
       "tau = 0.3 0 -0.028725  0.163289  0.149564  0.942\n",
       "          1 -0.000691  0.054713  0.047616  0.962\n",
       "          2  0.025672  0.068963  0.069358  0.918\n",
       "tau = 0.5 0 -0.009326  0.075044  0.068849  0.956\n",
       "          1 -0.006312  0.024788  0.020889  0.964\n",
       "          2  0.014723  0.031450  0.031365  0.918\n",
       "tau = 0.7 0 -0.004078  0.034028  0.031756  0.934\n",
       "          1 -0.003606  0.011559  0.010054  0.944\n",
       "          2  0.010519  0.014669  0.012923  0.940"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_set1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2db129-1b85-4b4e-bd57-83a8001b1161",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "450eabd0-eb9f-4116-a326-cd9fe4549716",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = setone_simulate(samplepath = 500,B = 200, C_u = 5,b1 = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e49ceee6-1a94-43dc-a5c2-8195ee645a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1691509501.240801\n",
      "程序运行时间:2999538.2816791534毫秒\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    T1 = time.time()\n",
    "    print(T1)\n",
    "    pool = mp.Pool(processes=40)\n",
    "    T1_ERR = pool.map(T1_hypothesis_testing_ERR,sample1)\n",
    "    T2 = time.time()\n",
    "    print('程序运行时间:%s毫秒' % ((T2 - T1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c5367b-9d06-4620-b906-cb057c6cdaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b1    0.910\n",
       "b2    0.226\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1_ERR = pd.DataFrame(T1_ERR)/500\n",
    "T1_ERR.columns = [\"b1\",\"b2\"]\n",
    "T1_ERR.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003da7e-96a7-4323-95e5-8ee1aba82c7e",
   "metadata": {},
   "source": [
    "# Average Covariate Effect Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62f888e2-e4ae-496f-8dc9-29d35914665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    T1 = time.time()\n",
    "    pool = mp.Pool(processes=40)\n",
    "    ace_var = pool.map(argdic_cov_effect_var,sample1)\n",
    "    all_betas = pool.map(argdic_quantreg_fit,sample1) \n",
    "    ace = []\n",
    "    ace_inference = pd.DataFrame()\n",
    "    for i in range(len(all_betas)):\n",
    "        ace.append(average_cov_effect(all_betas[i],grids,B,l_tau,u_tau))\n",
    "    ace = pd.DataFrame(ave)\n",
    "    ace_var = pd.DataFrame(ace_var)\n",
    "    ace_var.columns = [\"beta 0\", \"beta 1\", \"beta 2\"]\n",
    "    ace_inference[\"AveEst\"] = ace.mean()\n",
    "    ace_inference[\"EmpVar\"] = ace.var()\n",
    "    ace_inference[\"AveVar\"] = ace_var.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a7a537a4-362d-4ba8-a954-a9d0b82e9194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveEst</th>\n",
       "      <th>EmpVar</th>\n",
       "      <th>AveVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta 0</th>\n",
       "      <td>-0.011775</td>\n",
       "      <td>0.110948</td>\n",
       "      <td>0.101144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta 1</th>\n",
       "      <td>-0.511035</td>\n",
       "      <td>0.036787</td>\n",
       "      <td>0.035457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta 2</th>\n",
       "      <td>-0.306335</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.043887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AveEst    EmpVar    AveVar\n",
       "beta 0 -0.011775  0.110948  0.101144\n",
       "beta 1 -0.511035  0.036787  0.035457\n",
       "beta 2 -0.306335  0.044722  0.043887"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ace_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c85bb1-3897-4823-93c2-525e30b90526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
